{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc277b3c-4ed5-4c76-9d11-5bfe3cb2f944",
   "metadata": {},
   "source": [
    "#### 01.梯度下降loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00251cec-b007-4934-9483-13ff99956045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch,w is 2.600000,loss is 36.000000\n",
      "After 1 epoch,w is 1.160000,loss is 12.959999\n",
      "After 2 epoch,w is 0.296000,loss is 4.665599\n",
      "After 3 epoch,w is -0.222400,loss is 1.679616\n",
      "After 4 epoch,w is -0.533440,loss is 0.604662\n",
      "After 5 epoch,w is -0.720064,loss is 0.217678\n",
      "After 6 epoch,w is -0.832038,loss is 0.078364\n",
      "After 7 epoch,w is -0.899223,loss is 0.028211\n",
      "After 8 epoch,w is -0.939534,loss is 0.010156\n",
      "After 9 epoch,w is -0.963720,loss is 0.003656\n",
      "After 10 epoch,w is -0.978232,loss is 0.001316\n",
      "After 11 epoch,w is -0.986939,loss is 0.000474\n",
      "After 12 epoch,w is -0.992164,loss is 0.000171\n",
      "After 13 epoch,w is -0.995298,loss is 0.000061\n",
      "After 14 epoch,w is -0.997179,loss is 0.000022\n",
      "After 15 epoch,w is -0.998307,loss is 0.000008\n",
      "After 16 epoch,w is -0.998984,loss is 0.000003\n",
      "After 17 epoch,w is -0.999391,loss is 0.000001\n",
      "After 18 epoch,w is -0.999634,loss is 0.000000\n",
      "After 19 epoch,w is -0.999781,loss is 0.000000\n",
      "After 20 epoch,w is -0.999868,loss is 0.000000\n",
      "After 21 epoch,w is -0.999921,loss is 0.000000\n",
      "After 22 epoch,w is -0.999953,loss is 0.000000\n",
      "After 23 epoch,w is -0.999972,loss is 0.000000\n",
      "After 24 epoch,w is -0.999983,loss is 0.000000\n",
      "After 25 epoch,w is -0.999990,loss is 0.000000\n",
      "After 26 epoch,w is -0.999994,loss is 0.000000\n",
      "After 27 epoch,w is -0.999996,loss is 0.000000\n",
      "After 28 epoch,w is -0.999998,loss is 0.000000\n",
      "After 29 epoch,w is -0.999999,loss is 0.000000\n",
      "After 30 epoch,w is -0.999999,loss is 0.000000\n",
      "After 31 epoch,w is -1.000000,loss is 0.000000\n",
      "After 32 epoch,w is -1.000000,loss is 0.000000\n",
      "After 33 epoch,w is -1.000000,loss is 0.000000\n",
      "After 34 epoch,w is -1.000000,loss is 0.000000\n",
      "After 35 epoch,w is -1.000000,loss is 0.000000\n",
      "After 36 epoch,w is -1.000000,loss is 0.000000\n",
      "After 37 epoch,w is -1.000000,loss is 0.000000\n",
      "After 38 epoch,w is -1.000000,loss is 0.000000\n",
      "After 39 epoch,w is -1.000000,loss is 0.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "lr = 0.2\n",
    "epoch = 40\n",
    "\n",
    "for epoch in range(epoch):  # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。\n",
    "    with tf.GradientTape() as tape:  # with结构到grads框起了梯度的计算过程。\n",
    "        loss = tf.square(w + 1)\n",
    "    grads = tape.gradient(loss, w)  # .gradient函数告知谁对谁求导\n",
    "\n",
    "    w.assign_sub(lr * grads)  # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
    "    print(\"After %s epoch,w is %f,loss is %f\" % (epoch, w.numpy(), loss))\n",
    "\n",
    "# lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程\n",
    "# 最终目的：找到 loss 最小 即 w = -1 的最优参数w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc662a39-ecba-48d8-8db2-2ebd882c61b3",
   "metadata": {},
   "source": [
    "#### 02.张量"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2143253-8bdc-480a-b74e-3515541ba87a",
   "metadata": {},
   "source": [
    "数据类型\n",
    "tf.int,tf.float\n",
    "tf.int32,tf.float32,tf.float64\n",
    "\n",
    "tf.bool\n",
    "tf.sonstant([True,False])\n",
    "\n",
    "tf.string\n",
    "tf.constant(\"Hello\")\n",
    "\n",
    "创建一个张量\n",
    "tf.constant(张量内容，dtype=数据类型(可选))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211a4136-6fff-42d4-b419-723409924822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor([1 5], shape=(2,), dtype=int64)\n",
      "a.dtype: <dtype: 'int64'>\n",
      "a.shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 5], dtype=tf.int64)\n",
    "print(\"a:\", a)\n",
    "print(\"a.dtype:\", a.dtype)\n",
    "print(\"a.shape:\", a.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9dc2191-f40f-479b-afce-be38ed022868",
   "metadata": {},
   "source": [
    "将np的数据转为Tensor数据类型\n",
    "tf.convert_to_tensor(数据名，dtype=数据类型(可选))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc6ab43-58cc-4443-91bf-946d5dc3d7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [0 1 2 3 4]\n",
      "b: tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(0, 5)\n",
    "b = tf.convert_to_tensor(a, dtype=tf.int64)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ff173ea-c895-4c5f-a3e6-498984001767",
   "metadata": {},
   "source": [
    "创建全为0的张量\n",
    "tf.zeros(维度)\n",
    "\n",
    "创建全为1的张量\n",
    "tf.ones(维度)\n",
    "\n",
    "创建全为指定值的张量\n",
    "tf.fill(维度，指定值)\n",
    "\n",
    "维度：\n",
    "一维 直接写个数\n",
    "二维 用[行，列]\n",
    "多维 用[n,m,j,k,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491e8fd9-15fd-420f-9bd2-8da1b1960086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "b: tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "c: tf.Tensor(\n",
      "[[9 9]\n",
      " [9 9]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.zeros([2, 3])\n",
    "b = tf.ones(4)\n",
    "c = tf.fill([2, 2], 9)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6e3f4da-534e-4937-bd46-6d0905d4a2c4",
   "metadata": {},
   "source": [
    "生成正态分布的随机数，某人均值为0，标准差为1\n",
    "tf.random.normal(维度，mean=均值，stddev=标准差)\n",
    "\n",
    "生成截断式正态分布的随机数，某人均值为0，标准差为1\n",
    "tf.random.truncated_normal(维度，mean=均值，stddev=标准差) \n",
    "保证随机生成的数据在(u-2a,u+2a)之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6df161a-7f9e-4e49-8aac-9a51e2be5777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: tf.Tensor(\n",
      "[[-0.74730253  2.7275152 ]\n",
      " [ 1.3716078  -1.8315442 ]], shape=(2, 2), dtype=float32)\n",
      "e: tf.Tensor(\n",
      "[[-1.1083258  -0.32777256]\n",
      " [ 0.95055807  1.0771184 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d = tf.random.normal([2, 2], mean=0.5, stddev=1)\n",
    "print(\"d:\", d)\n",
    "e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=1)\n",
    "print(\"e:\", e)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d64e71f4-5a06-4369-938d-ac30c74dfcac",
   "metadata": {},
   "source": [
    "生成均匀分布随机数 [minval,maxval]\n",
    "tf.random.uniform(维度，minval=最小值，maxval=最大值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d42df64-21fe-4b82-99df-698110661776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: tf.Tensor(\n",
      "[[0.1133374  0.87342167]\n",
      " [0.46343374 0.41902196]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "f = tf.random.uniform([2, 2], minval=0, maxval=1)\n",
    "print(\"f:\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a3045-19f4-4ea6-90fa-b1f81c6e1b64",
   "metadata": {},
   "source": [
    "#### 03.常用函数"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19a11526-6e82-4b44-8ce4-ea5a981a5dbd",
   "metadata": {},
   "source": [
    "强制tensor转换为该数据类型\n",
    "tf.cast(张量名，dtype=数据类型)\n",
    "\n",
    "计算张量维度上元素的最小值\n",
    "tf.reduce_min(张量名)\n",
    "\n",
    "计算张量维度上元素的最大值\n",
    "tf.reduce_max(张量名)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143c005b-f573-4d40-a847-6f36eaf5d544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)\n",
      "x2 tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "minimum of x2： tf.Tensor(1, shape=(), dtype=int32)\n",
      "maxmum of x2: tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant([1., 2., 3.], dtype=tf.float64)\n",
    "print(\"x1:\", x1)\n",
    "x2 = tf.cast(x1, tf.int32)\n",
    "print(\"x2\", x2)\n",
    "print(\"minimum of x2：\", tf.reduce_min(x2))\n",
    "print(\"maxmum of x2:\", tf.reduce_max(x2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c66a2033-cff0-41e1-b521-ee9b84853089",
   "metadata": {},
   "source": [
    "axis=0:表示跨行（经度，down）方向操作\n",
    "axis=1:表示跨列（纬度，across）方向操作\n",
    "\n",
    "计算张量沿着指定纬度的平均值\n",
    "tf.reduce_mean(张量名,axis=操作轴)\n",
    "\n",
    "计算张量沿着指定纬度的和\n",
    "tf.reduce_sum(张量名,axis=操作轴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f5be17-9611-4dfe-97f1-afbed3e450b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 2 3]], shape=(2, 3), dtype=int32)\n",
      "mean of x: tf.Tensor(2, shape=(), dtype=int32)\n",
      "sum of x: tf.Tensor([6 7], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], [2, 2, 3]])\n",
    "print(\"x:\", x)\n",
    "print(\"mean of x:\", tf.reduce_mean(x))  # 求x中所有数的均值\n",
    "print(\"sum of x:\", tf.reduce_sum(x, axis=1))  # 求每一行的和"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5c0a86f-25ee-49cf-985e-a1ad144f2b7c",
   "metadata": {},
   "source": [
    "tf.Variable()可将变量标记为“可训练”，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记带训练参数\n",
    "tf.Variable(初始值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a9124c-75d8-43a9-b3c6-50a442f73f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal([2,2],mean=0,stddev=1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "178ec1d0-f656-45b1-b35c-a4d2f8c331e1",
   "metadata": {},
   "source": [
    "加减乘除：tf.add(张量1，张量2)，tf.subtract(张量1，张量2)，tf.multiply(张量1，张量2)，tf.divide(张量1，张量2)\n",
    "\n",
    "平方、次方、开方：tf.square(张量名)，tf.pow(张量名，n次方数)，tf.sqrt(张量名)\n",
    "\n",
    "矩阵乘：tf.matmul(矩阵1，矩阵2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02908d03-f744-4219-863f-af6c8c11cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)\n",
      "b: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "a+b: tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
      "a-b: tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)\n",
      "a*b: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "b/a: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones([1, 3])\n",
    "b = tf.fill([1, 3], 3.)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a+b:\", tf.add(a, b))\n",
    "print(\"a-b:\", tf.subtract(a, b))\n",
    "print(\"a*b:\", tf.multiply(a, b))\n",
    "print(\"b/a:\", tf.divide(b, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6d97c5-eb39-47dd-b39b-f0e214f70b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)\n",
      "a的平方: tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)\n",
      "a的平方: tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)\n",
      "a的开方: tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.fill([1, 2], 3.)\n",
    "print(\"a:\", a)\n",
    "print(\"a的平方:\", tf.pow(a, 3))\n",
    "print(\"a的平方:\", tf.square(a))\n",
    "print(\"a的开方:\", tf.sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28813c26-5c54-40fb-b99c-dad4d4a7ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]], shape=(3, 2), dtype=float32)\n",
      "b: tf.Tensor(\n",
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]], shape=(2, 3), dtype=float32)\n",
      "a*b: tf.Tensor(\n",
      "[[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones([3, 2])\n",
    "b = tf.fill([2, 3], 3.)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a*b:\", tf.matmul(a, b))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ceb9adf-2ebc-45cb-9694-a571a911d8a2",
   "metadata": {},
   "source": [
    "将输入特征与标签配对：\n",
    "data=tf.data.Dataset.from_tensor_slices((输入特征，标签))\n",
    "切分传入数据的第一个维度，生成输入特征/标签对，构建数据集。np和tensor格式均可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7c20c8-1831-41de-b011-bfdb35526fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "features = tf.constant([12, 23, 10, 17])\n",
    "labels = tf.constant([0, 1, 1, 0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83fc0656-11e2-44db-a1ab-3dfef2bf26d8",
   "metadata": {},
   "source": [
    "tf.GradientTape\n",
    "配合with结构记录计算过程，使用gradient求出张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047590ee-a6a3-4b1f-9c3c-bd368c7fb930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    x = tf.Variable(tf.constant(3.0))\n",
    "    y = tf.pow(x, 2)\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4478f740-fc69-485b-b32d-d01b218c2b2e",
   "metadata": {},
   "source": [
    "enumerate可遍历每个元素（列表，元组，字符串），组合为：索引 元素，配合for使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727b3160-13a0-4193-8d77-da75e41eaf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one\n",
      "1 two\n",
      "2 three\n"
     ]
    }
   ],
   "source": [
    "seq = ['one', 'two', 'three']\n",
    "for i, element in enumerate(seq):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2f61589-db9f-43dd-8c32-cf68410bce9e",
   "metadata": {},
   "source": [
    "独热编码\n",
    "tf.one_hot(待转换的数据，depth=几分类)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ff0707c-4288-40ec-901f-3d72c405e3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of labels1: tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = 3\n",
    "labels = tf.constant([1, 0, 2])  # 输入的元素值最小为0，最大为2\n",
    "output = tf.one_hot(labels, depth=classes)\n",
    "print(\"result of labels1:\", output)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9ff1634-594d-4b2e-89e2-1645deb6cd59",
   "metadata": {},
   "source": [
    "前向传播，tf.nn.softmax()求结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e4dd8d1-130d-449f-a80b-2fc5837815e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape: (1, 4)\n",
      "w1.shape: (4, 3)\n",
      "b1.shape: (3,)\n",
      "y.shape: (1, 3)\n",
      "y: tf.Tensor([[ 1.0099998   2.008      -0.65999985]], shape=(1, 3), dtype=float32)\n",
      "y_dim: tf.Tensor([ 1.0099998   2.008      -0.65999985], shape=(3,), dtype=float32)\n",
      "y_pro: tf.Tensor([0.2563381  0.69540703 0.04825491], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant([[5.8, 4.0, 1.2, 0.2]])  # 5.8,4.0,1.2,0.2（0）\n",
    "w1 = tf.constant([[-0.8, -0.34, -1.4],\n",
    "                  [0.6, 1.3, 0.25],\n",
    "                  [0.5, 1.45, 0.9],\n",
    "                  [0.65, 0.7, -1.2]])\n",
    "b1 = tf.constant([2.52, -3.1, 5.62])\n",
    "y = tf.matmul(x1, w1) + b1\n",
    "print(\"x1.shape:\", x1.shape)\n",
    "print(\"w1.shape:\", w1.shape)\n",
    "print(\"b1.shape:\", b1.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "print(\"y:\", y)\n",
    "\n",
    "#####以下代码可将输出结果y转化为概率值#####\n",
    "y_dim = tf.squeeze(y)  # 去掉y中纬度1（观察y_dim与 y 效果对比）\n",
    "y_pro = tf.nn.softmax(y_dim)  # 使y_dim符合概率分布，输出为概率值了\n",
    "print(\"y_dim:\", y_dim)\n",
    "print(\"y_pro:\", y_pro)\n",
    "\n",
    "#请观察打印出的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10baf698-821d-486a-863f-a3744838c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After softmax, y_pro is: tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n",
      "The sum of y_pro: tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([1.01, 2.01, -0.66])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "\n",
    "print(\"After softmax, y_pro is:\", y_pro)  # y_pro 符合概率分布\n",
    "\n",
    "print(\"The sum of y_pro:\", tf.reduce_sum(y_pro))  # 通过softmax后，所有概率加起来和为1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "216f6a74-837f-4aae-b6d2-dbb0c619c907",
   "metadata": {},
   "source": [
    "assign_sub\n",
    "幅值自减操作，更新参数的值并返回\n",
    "调用assign_sub之前，先用tf.Variable定义变量w为可训练(可自更新)\n",
    "w.assign_sub(w要自减的内容)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1f1c427-cb87-4004-a1f9-e9e39ee77c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(4)\n",
    "x.assign_sub(1)\n",
    "print(\"x:\", x)  # 4-1=3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a3a794e-716c-4397-bd22-e080c054890b",
   "metadata": {},
   "source": [
    "tf.argmax(张量名，axis=操作轴)\n",
    "返回张量沿制定维度最大值的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1901fbba-c9c0-45c1-b694-724e485d6c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      " [[1 2 3]\n",
      " [2 3 4]\n",
      " [5 4 3]\n",
      " [8 7 2]]\n",
      "每一列的最大值的索引： tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "每一行的最大值的索引 tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])\n",
    "print(\"test:\\n\", test)\n",
    "print(\"每一列的最大值的索引：\", tf.argmax(test, axis=0))  # 返回每一列最大值的索引\n",
    "print(\"每一行的最大值的索引\", tf.argmax(test, axis=1))  # 返回每一行最大值的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b443e00-4fd3-4cd7-9e4b-0746dd69659f",
   "metadata": {},
   "source": [
    "#### 04.鸢尾花数据分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3252b11-744c-4ed4-9046-1cc777d47fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data from datasets: \n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y_data from datasets: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "x_data add index: \n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
      "0         5.1       3.5       1.4       0.2\n",
      "1         4.9       3.0       1.4       0.2\n",
      "2         4.7       3.2       1.3       0.2\n",
      "3         4.6       3.1       1.5       0.2\n",
      "4         5.0       3.6       1.4       0.2\n",
      "..        ...       ...       ...       ...\n",
      "145       6.7       3.0       5.2       2.3\n",
      "146       6.3       2.5       5.0       1.9\n",
      "147       6.5       3.0       5.2       2.0\n",
      "148       6.2       3.4       5.4       2.3\n",
      "149       5.9       3.0       5.1       1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "x_data add a column: \n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
      "0         5.1       3.5       1.4       0.2     0\n",
      "1         4.9       3.0       1.4       0.2     0\n",
      "2         4.7       3.2       1.3       0.2     0\n",
      "3         4.6       3.1       1.5       0.2     0\n",
      "4         5.0       3.6       1.4       0.2     0\n",
      "..        ...       ...       ...       ...   ...\n",
      "145       6.7       3.0       5.2       2.3     2\n",
      "146       6.3       2.5       5.0       1.9     2\n",
      "147       6.5       3.0       5.2       2.0     2\n",
      "148       6.2       3.4       5.4       2.3     2\n",
      "149       5.9       3.0       5.1       1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "x_data = datasets.load_iris().data  # .data返回iris数据集所有输入特征\n",
    "y_data = datasets.load_iris().target  # .target返回iris数据集所有标签\n",
    "print(\"x_data from datasets: \\n\", x_data)\n",
    "print(\"y_data from datasets: \\n\", y_data)\n",
    "\n",
    "x_data = DataFrame(x_data, columns=['花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度']) # 为表格增加行索引（左侧）和列标签（上方）\n",
    "pd.set_option('display.unicode.east_asian_width', True)  # 设置列名对齐\n",
    "print(\"x_data add index: \\n\", x_data)\n",
    "\n",
    "x_data['类别'] = y_data  # 新加一列，列标签为‘类别’，数据为y_data\n",
    "print(\"x_data add a column: \\n\", x_data)\n",
    "\n",
    "#类型维度不确定时，建议用print函数打印出来确认效果"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7703e82a-1201-42c6-a6a3-a21717de4309",
   "metadata": {},
   "source": [
    "1.准备数据\n",
    "  -数据集读入\n",
    "  -数据集乱序\n",
    "  -生成训练集和测试集（即x_train/y_train,x_test/y_test）\n",
    "  -配成（输入特征，标签）对，每次读入一小撮（batch）\n",
    "  \n",
    "2.搭建网络\n",
    "  -定义神经网络中所有可训练的参数\n",
    "  \n",
    "3.参数优化\n",
    "  -嵌套循环迭代，with结构更新参数，显示当前loss\n",
    "  \n",
    "4.测试效果\n",
    "  -计算当前参数前向传播后的准确率，显示当前acc\n",
    "  \n",
    "5.acc/loss可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f8a170f-b7d5-4871-a479-c8ad214b31ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2821310982108116\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 1, loss: 0.25459614023566246\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 2, loss: 0.22570250183343887\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 3, loss: 0.21028400212526321\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 4, loss: 0.19942265003919601\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 5, loss: 0.18873638287186623\n",
      "Test_acc: 0.5\n",
      "--------------------------\n",
      "Epoch 6, loss: 0.17851299419999123\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 7, loss: 0.16922875493764877\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 8, loss: 0.16107673197984695\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 9, loss: 0.15404684096574783\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 10, loss: 0.14802725985646248\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 11, loss: 0.14287303388118744\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 12, loss: 0.1384414155036211\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 13, loss: 0.13460607267916203\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 14, loss: 0.1312607266008854\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 15, loss: 0.12831821851432323\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 16, loss: 0.12570794858038425\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 17, loss: 0.12337299063801765\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 18, loss: 0.12126746959984303\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 19, loss: 0.11935433000326157\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 20, loss: 0.11760355532169342\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 21, loss: 0.11599067784845829\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 22, loss: 0.11449568346142769\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 23, loss: 0.11310208030045033\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 24, loss: 0.11179621517658234\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 25, loss: 0.11056671850383282\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 26, loss: 0.1094040796160698\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 27, loss: 0.10830028168857098\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 28, loss: 0.10724855586886406\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 29, loss: 0.10624313727021217\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 30, loss: 0.1052791029214859\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 31, loss: 0.10435222089290619\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 32, loss: 0.10345886647701263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 33, loss: 0.10259587690234184\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 34, loss: 0.10176053084433079\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 35, loss: 0.10095042362809181\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 36, loss: 0.10016347281634808\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 37, loss: 0.09939785301685333\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 38, loss: 0.098651934415102\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 39, loss: 0.09792428836226463\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 40, loss: 0.09721364639699459\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 41, loss: 0.09651889279484749\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 42, loss: 0.09583901055157185\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 43, loss: 0.09517310746014118\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 44, loss: 0.09452036395668983\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 45, loss: 0.0938800685107708\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 46, loss: 0.09325156174600124\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 47, loss: 0.09263424947857857\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 48, loss: 0.09202760085463524\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 49, loss: 0.09143111668527126\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 50, loss: 0.09084436297416687\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 51, loss: 0.09026693738996983\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 52, loss: 0.08969846740365028\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 53, loss: 0.08913861028850079\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 54, loss: 0.08858705312013626\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 55, loss: 0.08804351277649403\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 56, loss: 0.08750772476196289\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 57, loss: 0.08697944693267345\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 58, loss: 0.08645843341946602\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 59, loss: 0.08594449236989021\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 60, loss: 0.08543741516768932\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 61, loss: 0.08493702113628387\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 62, loss: 0.08444313704967499\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 63, loss: 0.08395560085773468\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 64, loss: 0.08347426354885101\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 65, loss: 0.08299897983670235\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 66, loss: 0.08252961002290249\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 67, loss: 0.08206603676080704\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 68, loss: 0.0816081278026104\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 69, loss: 0.08115577697753906\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 70, loss: 0.08070887438952923\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 71, loss: 0.08026730641722679\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 72, loss: 0.07983098179101944\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 73, loss: 0.07939981482923031\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 74, loss: 0.07897369377315044\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 75, loss: 0.07855254411697388\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 76, loss: 0.07813627645373344\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 77, loss: 0.07772481068968773\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 78, loss: 0.07731806486845016\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 79, loss: 0.07691597566008568\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 80, loss: 0.07651845179498196\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 81, loss: 0.07612544111907482\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 82, loss: 0.07573685608804226\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 83, loss: 0.07535265013575554\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 84, loss: 0.07497275061905384\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 85, loss: 0.07459708210080862\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 86, loss: 0.07422559335827827\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 87, loss: 0.07385822758078575\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 88, loss: 0.07349492330104113\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 89, loss: 0.0731356181204319\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 90, loss: 0.0727802598848939\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 91, loss: 0.07242879830300808\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 92, loss: 0.07208118122071028\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 93, loss: 0.07173734251409769\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 94, loss: 0.07139723561704159\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 95, loss: 0.07106082048267126\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 96, loss: 0.07072803843766451\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 97, loss: 0.07039883732795715\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 98, loss: 0.07007318176329136\n",
      "Test_acc: 0.8333333333333334\n",
      "--------------------------\n",
      "Epoch 99, loss: 0.0697510102763772\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 100, loss: 0.06943229492753744\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 101, loss: 0.06911696959286928\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 102, loss: 0.06880500260740519\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 103, loss: 0.068496348336339\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 104, loss: 0.06819095741957426\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 105, loss: 0.06788879353553057\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 106, loss: 0.06758981756865978\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 107, loss: 0.0672939782962203\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 108, loss: 0.06700124125927687\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 109, loss: 0.06671155989170074\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 110, loss: 0.06642490718513727\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 111, loss: 0.06614123564213514\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 112, loss: 0.06586050800979137\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 113, loss: 0.06558268237859011\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 114, loss: 0.06530772428959608\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 115, loss: 0.06503560300916433\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 116, loss: 0.06476627010852098\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 117, loss: 0.06449970323592424\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 118, loss: 0.06423585396260023\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 119, loss: 0.06397469528019428\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 120, loss: 0.06371619179844856\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 121, loss: 0.06346031185239553\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 122, loss: 0.06320700887590647\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 123, loss: 0.06295627169311047\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 124, loss: 0.06270804442465305\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 125, loss: 0.062462314032018185\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 126, loss: 0.062219033017754555\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 127, loss: 0.061978185549378395\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 128, loss: 0.06173973437398672\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 129, loss: 0.06150364130735397\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 130, loss: 0.06126988586038351\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 131, loss: 0.06103843078017235\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 132, loss: 0.060809263959527016\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 133, loss: 0.06058233231306076\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 134, loss: 0.06035762373358011\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 135, loss: 0.06013510562479496\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 136, loss: 0.05991474352777004\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 137, loss: 0.05969652719795704\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 138, loss: 0.05948041472584009\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 139, loss: 0.059266386553645134\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 140, loss: 0.059054408222436905\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 141, loss: 0.058844465762376785\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 142, loss: 0.05863652750849724\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 143, loss: 0.058430563658475876\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 144, loss: 0.058226559311151505\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 145, loss: 0.05802448187023401\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 146, loss: 0.05782431084662676\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 147, loss: 0.0576260257512331\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 148, loss: 0.05742959305644035\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 149, loss: 0.057234992273151875\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 150, loss: 0.05704221595078707\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 151, loss: 0.05685121938586235\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 152, loss: 0.05666199326515198\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 153, loss: 0.05647451523691416\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 154, loss: 0.0562887629494071\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 155, loss: 0.05610471125692129\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 156, loss: 0.05592234432697296\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 157, loss: 0.0557416332885623\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 158, loss: 0.05556256324052811\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 159, loss: 0.05538512021303177\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 160, loss: 0.05520927160978317\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 161, loss: 0.0550350034609437\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 162, loss: 0.054862307384610176\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 163, loss: 0.05469114426523447\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 164, loss: 0.05452151037752628\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 165, loss: 0.05435337871313095\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 166, loss: 0.05418673437088728\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 167, loss: 0.054021554067730904\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 168, loss: 0.053857832215726376\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 169, loss: 0.05369554739445448\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 170, loss: 0.05353467632085085\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 171, loss: 0.05337520316243172\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 172, loss: 0.05321711581200361\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 173, loss: 0.053060390055179596\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 174, loss: 0.05290501844137907\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 175, loss: 0.05275098513811827\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 176, loss: 0.0525982566177845\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 177, loss: 0.05244683939963579\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 178, loss: 0.05229670740664005\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 179, loss: 0.05214785039424896\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 180, loss: 0.052000246942043304\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 181, loss: 0.05185388680547476\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 182, loss: 0.05170875135809183\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 183, loss: 0.0515648303553462\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 184, loss: 0.0514221116900444\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 185, loss: 0.05128058046102524\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 186, loss: 0.05114021524786949\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 187, loss: 0.051001012325286865\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 188, loss: 0.0508629409596324\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 189, loss: 0.05072600767016411\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 190, loss: 0.05059019848704338\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 191, loss: 0.05045548640191555\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 192, loss: 0.050321875140070915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 193, loss: 0.05018933489918709\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 194, loss: 0.05005786381661892\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 195, loss: 0.04992745537310839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 196, loss: 0.04979807883501053\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 197, loss: 0.04966974165290594\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 198, loss: 0.04954242426902056\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 199, loss: 0.04941611457616091\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 200, loss: 0.049290805123746395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 201, loss: 0.049166472628712654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 202, loss: 0.04904312454164028\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 203, loss: 0.04892073106020689\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 204, loss: 0.04879929404705763\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 205, loss: 0.04867880046367645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 206, loss: 0.04855923913419247\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 207, loss: 0.048440598882734776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 208, loss: 0.04832286946475506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 209, loss: 0.04820604622364044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 210, loss: 0.04809010960161686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 211, loss: 0.04797505959868431\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 212, loss: 0.04786087851971388\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 213, loss: 0.047747560776770115\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 214, loss: 0.04763508960604668\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 215, loss: 0.04752346687018871\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 216, loss: 0.0474126823246479\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 217, loss: 0.04730272572487593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 218, loss: 0.04719358030706644\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 219, loss: 0.04708524979650974\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 220, loss: 0.046977708116173744\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 221, loss: 0.046870965510606766\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 222, loss: 0.04676500242203474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 223, loss: 0.046659816056489944\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 224, loss: 0.046555391512811184\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 225, loss: 0.04645173158496618\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 226, loss: 0.046348825097084045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 227, loss: 0.04624664504081011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 228, loss: 0.046145214699208736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 229, loss: 0.04604450333863497\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 230, loss: 0.04594451282173395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 231, loss: 0.045845234766602516\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 232, loss: 0.04574666079133749\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 233, loss: 0.04564878437668085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 234, loss: 0.04555159714072943\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 235, loss: 0.04545509163290262\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 236, loss: 0.045359269715845585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 237, loss: 0.04526410344988108\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 238, loss: 0.04516960680484772\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 239, loss: 0.04507576581090689\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 240, loss: 0.044982570223510265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 241, loss: 0.04489001724869013\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 242, loss: 0.04479810781776905\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 243, loss: 0.04470681864768267\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 244, loss: 0.04461614973843098\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 245, loss: 0.04452611040323973\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 246, loss: 0.04443667363375425\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 247, loss: 0.044347839429974556\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 248, loss: 0.04425961058586836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 249, loss: 0.04417197220027447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 250, loss: 0.044084908440709114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 251, loss: 0.043998440727591515\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 252, loss: 0.04391253925859928\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 253, loss: 0.043827205896377563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 254, loss: 0.04374244436621666\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 255, loss: 0.04365824069827795\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 256, loss: 0.04357459116727114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 257, loss: 0.043491488322615623\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 258, loss: 0.043408920988440514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 259, loss: 0.043326896615326405\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 260, loss: 0.04324540589004755\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 261, loss: 0.043164435774087906\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 262, loss: 0.04308399651199579\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 263, loss: 0.043004062958061695\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 264, loss: 0.04292465187609196\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 265, loss: 0.04284573998302221\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 266, loss: 0.04276733938604593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 267, loss: 0.042689427733421326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 268, loss: 0.042612009681761265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 269, loss: 0.042535084299743176\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 270, loss: 0.04245864413678646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 271, loss: 0.0423826826736331\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 272, loss: 0.042307195253670216\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 273, loss: 0.04223217722028494\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 274, loss: 0.0421576201915741\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 275, loss: 0.042083533480763435\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 276, loss: 0.04200989939272404\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 277, loss: 0.041936714202165604\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 278, loss: 0.041863986290991306\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 279, loss: 0.04179169982671738\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 280, loss: 0.041719854809343815\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 281, loss: 0.041648441925644875\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 282, loss: 0.04157746955752373\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 283, loss: 0.04150692094117403\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 284, loss: 0.04143680352717638\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 285, loss: 0.04136709962040186\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 286, loss: 0.04129782039672136\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 287, loss: 0.04122895374894142\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 288, loss: 0.04116049408912659\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 289, loss: 0.04109244793653488\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 290, loss: 0.04102479945868254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 291, loss: 0.04095755238085985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 292, loss: 0.040890694595873356\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 293, loss: 0.040824233554303646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 294, loss: 0.040758166462183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 295, loss: 0.040692479349672794\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 296, loss: 0.04062717128545046\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 297, loss: 0.040562248788774014\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 298, loss: 0.04049769788980484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 299, loss: 0.04043351951986551\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 300, loss: 0.04036970995366573\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 301, loss: 0.040306271985173225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 302, loss: 0.04024319350719452\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 303, loss: 0.04018046986311674\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 304, loss: 0.040118103846907616\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 305, loss: 0.04005609406158328\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 306, loss: 0.03999443957582116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 307, loss: 0.03993312222883105\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 308, loss: 0.039872155059129\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 309, loss: 0.03981153108179569\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 310, loss: 0.03975124144926667\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 311, loss: 0.03969129454344511\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 312, loss: 0.03963167825713754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 313, loss: 0.039572385139763355\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 314, loss: 0.0395134249702096\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 315, loss: 0.039454787503927946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 316, loss: 0.039396482054144144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 317, loss: 0.03933848813176155\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 318, loss: 0.03928081039339304\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 319, loss: 0.039223446510732174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 320, loss: 0.039166401606053114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 321, loss: 0.039109662640839815\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 322, loss: 0.03905322263017297\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 323, loss: 0.03899709461256862\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 324, loss: 0.03894126648083329\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 325, loss: 0.038885737769305706\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 326, loss: 0.03883049916476011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 327, loss: 0.03877555998042226\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 328, loss: 0.03872091369703412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 329, loss: 0.038666556123644114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 330, loss: 0.0386124849319458\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 331, loss: 0.038558701518923044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 332, loss: 0.03850520169362426\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 333, loss: 0.03845197660848498\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 334, loss: 0.03839903511106968\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 335, loss: 0.03834636742249131\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 336, loss: 0.03829397866502404\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 337, loss: 0.038241852074861526\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 338, loss: 0.03819000208750367\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 339, loss: 0.03813841659575701\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 340, loss: 0.038087102584540844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 341, loss: 0.03803604608401656\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 342, loss: 0.037985255010426044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 343, loss: 0.0379347144626081\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 344, loss: 0.03788443887606263\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 345, loss: 0.037834418937563896\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 346, loss: 0.03778465045616031\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 347, loss: 0.03773513110354543\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 348, loss: 0.03768586413934827\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 349, loss: 0.03763684118166566\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 350, loss: 0.037588071543723345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 351, loss: 0.037539539858698845\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 352, loss: 0.03749124659225345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 353, loss: 0.03744320431724191\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 354, loss: 0.03739539487287402\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 355, loss: 0.037347821053117514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 356, loss: 0.037300472147762775\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 357, loss: 0.03725337469950318\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 358, loss: 0.03720649937167764\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 359, loss: 0.037159846629947424\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 360, loss: 0.03711343090981245\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 361, loss: 0.03706724522635341\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 362, loss: 0.03702127141878009\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 363, loss: 0.036975531373173\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 364, loss: 0.036930006463080645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 365, loss: 0.036884702276438475\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 366, loss: 0.03683961182832718\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 367, loss: 0.036794747691601515\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 368, loss: 0.03675009263679385\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 369, loss: 0.03670565178617835\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 370, loss: 0.03666142327710986\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 371, loss: 0.036617396865040064\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 372, loss: 0.03657358651980758\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 373, loss: 0.03652997827157378\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 374, loss: 0.03648658422753215\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 375, loss: 0.03644339134916663\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 376, loss: 0.03640039125457406\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 377, loss: 0.03635760257020593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 378, loss: 0.0363150117918849\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 379, loss: 0.03627262031659484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 380, loss: 0.036230423022061586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 381, loss: 0.03618842549622059\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 382, loss: 0.03614661982282996\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 383, loss: 0.036105002742260695\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 384, loss: 0.03606357332319021\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 385, loss: 0.036022343672811985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 386, loss: 0.03598130075260997\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 387, loss: 0.0359404431656003\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 388, loss: 0.035899775102734566\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 389, loss: 0.03585928911343217\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 390, loss: 0.035818991251289845\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 391, loss: 0.03577886475250125\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 392, loss: 0.035738930106163025\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 393, loss: 0.035699169617146254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 394, loss: 0.03565958747640252\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 395, loss: 0.0356201883405447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 396, loss: 0.03558096336200833\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 397, loss: 0.035541911609470844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 398, loss: 0.035503033082932234\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 399, loss: 0.03546432685106993\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 400, loss: 0.03542578825727105\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 401, loss: 0.03538742894306779\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 402, loss: 0.03534923028200865\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 403, loss: 0.03531120624393225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 404, loss: 0.03527334099635482\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 405, loss: 0.035235646180808544\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 406, loss: 0.035198114812374115\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 407, loss: 0.03516074409708381\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 408, loss: 0.03512354753911495\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 409, loss: 0.035086496733129025\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 410, loss: 0.03504961961880326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 411, loss: 0.03501288779079914\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 412, loss: 0.03497632406651974\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 413, loss: 0.034939910750836134\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 414, loss: 0.034903660882264376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 415, loss: 0.03486755723133683\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 416, loss: 0.034831615164875984\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 417, loss: 0.03479582257568836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 418, loss: 0.034760179463773966\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 419, loss: 0.03472469002008438\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 420, loss: 0.034689351450651884\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 421, loss: 0.03465416468679905\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 422, loss: 0.03461912088096142\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 423, loss: 0.034584226086735725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 424, loss: 0.0345494719222188\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 425, loss: 0.03451487235724926\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 426, loss: 0.034480408765375614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 427, loss: 0.03444609651342034\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 428, loss: 0.03441191930323839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 429, loss: 0.034377888310700655\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 430, loss: 0.0343439974822104\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 431, loss: 0.03431024495512247\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 432, loss: 0.034276632592082024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 433, loss: 0.03424314921721816\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 434, loss: 0.034209814853966236\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 435, loss: 0.034176611341536045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 436, loss: 0.03414354659616947\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 437, loss: 0.034110613632947206\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 438, loss: 0.03407781524583697\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 439, loss: 0.03404514491558075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 440, loss: 0.03401261428371072\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 441, loss: 0.03398020705208182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 442, loss: 0.03394793579354882\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 443, loss: 0.03391578933224082\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 444, loss: 0.033883774653077126\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 445, loss: 0.03385189222171903\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 446, loss: 0.03382013039663434\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 447, loss: 0.033788496162742376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 448, loss: 0.033756992779672146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 449, loss: 0.03372560581192374\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 450, loss: 0.03369434829801321\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 451, loss: 0.03366321278735995\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 452, loss: 0.03363220160827041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 453, loss: 0.03360130963847041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 454, loss: 0.033570545725524426\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 455, loss: 0.03353988938033581\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 456, loss: 0.03350936621427536\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 457, loss: 0.03347895201295614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 458, loss: 0.03344865795224905\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 459, loss: 0.033418478444218636\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 460, loss: 0.033388426061719656\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 461, loss: 0.033358484506607056\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 462, loss: 0.033328657038509846\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 463, loss: 0.033298940397799015\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 464, loss: 0.033269339706748724\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 465, loss: 0.03323985496535897\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 466, loss: 0.03321048337966204\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 467, loss: 0.03318122075870633\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 468, loss: 0.0331520764157176\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 469, loss: 0.033123028464615345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 470, loss: 0.033094107173383236\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 471, loss: 0.033065286464989185\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 472, loss: 0.0330365770496428\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 473, loss: 0.03300797566771507\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 474, loss: 0.03297947999089956\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 475, loss: 0.03295109001919627\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 476, loss: 0.03292280342429876\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 477, loss: 0.03289463231340051\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 478, loss: 0.0328665585257113\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 479, loss: 0.03283859835937619\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 480, loss: 0.03281073085963726\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 481, loss: 0.03278297046199441\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 482, loss: 0.032755312509834766\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 483, loss: 0.03272775420919061\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 484, loss: 0.03270029416307807\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 485, loss: 0.032672947738319635\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 486, loss: 0.032645690720528364\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 487, loss: 0.03261853428557515\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 488, loss: 0.03259148681536317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 489, loss: 0.03256453387439251\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 490, loss: 0.032537673600018024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 491, loss: 0.03251091670244932\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 492, loss: 0.03248425014317036\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 493, loss: 0.0324576823040843\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 494, loss: 0.032431211322546005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 495, loss: 0.03240483347326517\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 496, loss: 0.03237855713814497\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 497, loss: 0.03235236741602421\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 498, loss: 0.03232627175748348\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 499, loss: 0.0323002771474421\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
    "\n",
    "# 导入所需模块\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 导入数据，分别为输入特征和标签\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
    "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
    "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "\n",
    "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
    "# 用tf.Variable()标记参数可训练\n",
    "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
    "\n",
    "lr = 0.1  # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 500  # 循环500轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
    "\n",
    "# 训练部分\n",
    "for epoch in range(epoch):  #数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  #batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "\n",
    "    # 测试部分\n",
    "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        # 使用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
    "        # 将pred转换为y_test的数据类型\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        # 将每个batch的correct数加起来\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        # 将所有batch中的correct数加起来\n",
    "        total_correct += int(correct)\n",
    "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bff7c238-f125-4a7d-a522-4f217ccadc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 4), (None,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
